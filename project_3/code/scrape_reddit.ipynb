{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5906268-5ba2-43dd-856a-00814cece038",
   "metadata": {},
   "source": [
    "## Scrape Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88779929-7a9f-4c7f-b5e5-44f7f6c8c2fc",
   "metadata": {},
   "source": [
    "Now, imagine that I am working for a gaming news platform.  \n",
    "My platform is rolling out a new service, pushing interesting contents from a bank of mixed up posts to user, based on ther user's preference.  \n",
    "Before I push the post to the Nintendo/ PlayStation fans, I will first need to identify where these posts come from.  \n",
    "Somehow the contents the game news platform generates is very similar to reddit, so I decided to gather some reddit posts and make a classifier which can identify is the post is from `r/nintendo` or `r/playstation`.  \n",
    "\n",
    "---\n",
    "\n",
    "For this notebook, my goal is:  \n",
    "Collect posts from two subreddits. `r/nintendo` and `r/playstation`, using `praw`, [The Python Reddit API Wrapper](https://praw.readthedocs.io/en/stable/#praw-the-python-reddit-api-wrapper).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "689f24d2-ae1e-42ad-8002-b76609162b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import random\n",
    "import pandas as pd\n",
    "from os.path import isfile\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97f6ae5-6bc3-4deb-b3c5-0079e020764e",
   "metadata": {},
   "source": [
    "Write a class to scrape reddit.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f41c1d3d-13ac-43ba-95f2-6cf2fb6cde66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapeAndMakeCSV(object):\n",
    "    '''This class is to scrape reddit and make a CSV at the end of it\n",
    "    Args:\n",
    "        sub: name of subreddit to scrape\n",
    "        sort_by: how to sort subreddit, default = hot\n",
    "        num: number of post to gather\n",
    "        save_path: path to save the csv\n",
    "    '''\n",
    "    def __init__(self, sub, sort_by='hot', num=1000, save_path = '../data/'):\n",
    "        self.sub = sub\n",
    "        self.sort_by = sort_by\n",
    "        self.num = num\n",
    "        self.save_path = save_path\n",
    "        self.bot = praw.Reddit('Bot1') # Initialise bot using praw.ini\n",
    "        print(f'Initilised API call for\\n scrapping: {self.sub},\\n sorted by: {self.sort_by},\\n for: {self.num} posts.')\n",
    "        \n",
    "    def set_sort(self):\n",
    "        '''Set how to sort subreddit posts. Default = hot\n",
    "        '''\n",
    "        reddit = self.bot\n",
    "        if self.sort_by == 'hot':\n",
    "            posts =  reddit.subreddit(self.sub).hot(limit=self.num)\n",
    "        if self.sort_by == 'new':\n",
    "            posts =  reddit.subreddit(self.sub).new(limit=self.num)\n",
    "        elif self.sort_by == 'top':\n",
    "            posts =  reddit.subreddit(self.sub).top(limit=self.num)\n",
    "        else:\n",
    "            self.sort_by = 'hot'\n",
    "            print('Sort method was not recognized, defaulting to hot.')\n",
    "            posts =  reddit.subreddit(self.sub).hot(limit=self.num)\n",
    "        print(f'Calling to: {self.sub}, with posts sorted by: {self.sort_by}, gathering {self.num} posts.')\n",
    "        return posts\n",
    "    \n",
    "    def get_help(self):\n",
    "        '''Run the help function so that\n",
    "        '''\n",
    "        help_post = self.set_sort()\n",
    "        help(help_post)\n",
    "    \n",
    "    def make_df(self):\n",
    "        '''Makes a DataFrame with posts gathered\n",
    "        '''\n",
    "        # Make a dataframe dict\n",
    "        new_posts_dict = {\n",
    "            'id': [],\n",
    "            'title': [],\n",
    "            'post_content': [], \n",
    "        }\n",
    "        \n",
    "        csv = f'{self.save_path}{self.sub}_posts.csv'\n",
    "        \n",
    "        # Set csv_loaded to True if csv exists since you can't \n",
    "        # evaluate the truth value of a DataFrame.\n",
    "        df, csv_loaded = (pd.read_csv(csv), True) if isfile(csv) else ('', False)\n",
    "        \n",
    "        print(f'csv_loaded = {csv_loaded}')\n",
    "        \n",
    "        # Get post from 3 possible sorts methods\n",
    "        posts = self.set_sort()\n",
    "        \n",
    "        for post in posts:\n",
    "            # Check if post.id is in df and set to True if df is empty.\n",
    "            # This way new posts are still added to dictionary when df = '' \n",
    "            unique_id = post.id not in tuple(df.id) if csv_loaded else True\n",
    "            \n",
    "            # Save any unique posts to sub_dict.\n",
    "            if unique_id:\n",
    "                new_posts_dict['id'].append(post.id)\n",
    "                new_posts_dict['title'].append(post.title)\n",
    "                new_posts_dict['post_content'].append(post.selftext)\n",
    "            # Sleep for short while\n",
    "            sleep(random.randint(0)) # This is probably not needed, remove if you want code to run fast\n",
    "            \n",
    "        # Make new dataframe\n",
    "        new_df = pd.DataFrame(new_posts_dict)\n",
    "        # Add new_df to df if df exists then save it to a csv.\n",
    "        if 'DataFrame' in str(type(df)):\n",
    "            pd.concat([df, new_df], axis=False, sort=False).to_csv(csv, index=False)\n",
    "            print(f'{len(new_df)} new posts collected and added to {csv}')\n",
    "        else:\n",
    "            new_df.to_csv(csv, index=False)\n",
    "            print(f'{len(new_df)} posts collected and saved to {csv}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f84aa367-8ffc-4885-8084-c87fb5e0c030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initilised API call for\n",
      " scrapping: playstation,\n",
      " sorted by: hot,\n",
      " for: 1500 posts.\n"
     ]
    }
   ],
   "source": [
    "posts = ScrapeAndMakeCSV('playstation','hot', num=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f8222ee0-44c4-469a-a434-e02e204436a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv_loaded = True\n",
      "Sort method was not recognized, defaulting to hot.\n",
      "Calling to: playstation, with posts sorted by: hot, gathering 1500 posts.\n",
      "924 new posts collected and added to ../data/playstation_posts.csv\n"
     ]
    }
   ],
   "source": [
    "posts.make_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b16cae-9acf-49e2-95b0-e65c0bf94ab6",
   "metadata": {},
   "source": [
    "These data will then be saved into 2 different csv files under the `data` folder.  \n",
    "They are:  \n",
    "[nintendo_posts.csv](./data/nintendo_posts.csv)  \n",
    "[playstation_posts.csv](./data/playstation_posts.csv)\n",
    "\n",
    "### Data dictionary\n",
    "|no|feature|description|\n",
    "|-|-|-|\n",
    "|1|id|the unique id to each reddit post, used to make only 1 post is scrapped|\n",
    "|2|title|title of the post scrapped|\n",
    "|3|post_content|if the post is word post, then post content exit|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "152cb776-7627-402f-b7cb-2e0ee02fb78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1910, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps_df = pd.read_csv('../data/playstation_posts.csv')\n",
    "ps_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "11221665-77be-4ded-ba62-06881b4e2a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1916, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns_df = pd.read_csv('../data/nintendo_posts.csv')\n",
    "ns_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f41a1d6-09a1-4706-83f0-5562a0e16bb0",
   "metadata": {},
   "source": [
    "With this, I have collected about 1900 posts each from the reddit forums. \n",
    "Now, I will be able to start my EDA and prototype model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi-sg]",
   "language": "python",
   "name": "conda-env-dsi-sg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
